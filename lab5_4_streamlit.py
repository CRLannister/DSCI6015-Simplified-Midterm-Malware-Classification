import json
import boto3
import streamlit as st
import pandas as pd

import collections
from nltk import ngrams
import numpy as np
import pefile
import joblib
from scipy.sparse import csr_matrix, hstack

#password@123
N=2
def readFile(filePath):
    with open(filePath, "rb") as binary_file:
        data = binary_file.read()
    return data

def byteSequenceToNgrams(byteSequence, n):
    Ngrams = ngrams(byteSequence, n)
    return list(Ngrams)

def extractNgramCounts(file, N):
    fileByteSequence = readFile(file)
    fileNgrams = byteSequenceToNgrams(fileByteSequence, N)
    return collections.Counter(fileNgrams)

def getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list):
    K1 = len(K1_most_common_Ngrams_list)
    fv = K1*[0]
    fileNgrams = extractNgramCounts(file, N)
    for i in range(K1):
        fv[i]=fileNgrams[K1_most_common_Ngrams_list[i]]
    return fv

def preprocessImports(listOfDLLs):
    temp = [x.decode().split(".")[0].lower() for x in listOfDLLs]
    return " ".join(temp)

def getImports(pe):
    listOfImports = []
    for entry in pe.DIRECTORY_ENTRY_IMPORT:
        listOfImports.append(entry.dll)
    return preprocessImports(listOfImports)

def getSectionNames(pe):
    listOfSectionNames = []
    for eachSection in pe.sections:
        refined_name = eachSection.Name.decode().replace('\x00','').lower()
        listOfSectionNames.append(refined_name)
    return " ".join(listOfSectionNames)



K1_most_common_Ngrams_list = [(0, 0),
 (255, 255),
 (204, 204),
 (2, 100),
 (1, 0),
 (0, 139),
 (131, 196),
 (2, 0),
 (68, 36),
 (139, 69),
 (0, 131),
 (255, 117),
 (133, 192),
 (255, 139),
 (254, 255),
 (46, 46),
 (139, 77),
 (141, 77),
 (7, 0),
 (255, 21),
 (69, 252),
 (0, 1),
 (76, 36),
 (8, 139),
 (4, 0),
 (137, 69),
 (4, 139),
 (141, 69),
 (255, 131),
 (0, 255),
 (0, 137),
 (51, 192),
 (80, 232),
 (255, 141),
 (85, 139),
 (8, 0),
 (0, 232),
 (0, 116),
 (15, 182),
 (80, 141),
 (3, 100),
 (139, 236),
 (100, 0),
 (15, 132),
 (12, 139),
 (255, 0),
 (64, 0),
 (253, 255),
 (84, 36),
 (65, 68),
 (73, 78),
 (80, 65),
 (68, 68),
 (78, 71),
 (68, 73),
 (0, 204),
 (16, 0),
 (198, 69),
 (192, 116),
 (199, 69),
 (3, 0),
 (80, 255),
 (4, 137),
 (139, 68),
 (204, 139),
 (116, 36),
 (101, 0),
 (106, 0),
 (64, 2),
 (139, 76),
 (0, 8),
 (2, 101),
 (196, 12),
 (100, 139),
 (139, 70),
 (36, 8),
 (69, 8),
 (117, 8),
 (196, 4),
 (32, 0),
 (0, 89),
 (86, 139),
 (95, 94),
 (0, 16),
 (131, 192),
 (0, 80),
 (0, 117),
 (0, 141),
 (139, 255),
 (100, 232),
 (0, 72),
 (36, 20),
 (195, 204),
 (9, 0),
 (0, 128),
 (139, 240),
 (36, 16),
 (6, 0),
 (1, 100),
 (0, 15)]


def classify(payload_data):
    client = boto3.client('sagemaker-runtime',
                          aws_access_key_id='ASIA2SPULRZBUG7HRWGE',
                          aws_secret_access_key='M5DKW8DXrvlaZd9hQocjULhZNp4SeSBu0dwLMo+l',
                          aws_session_token='IQoJb3JpZ2luX2VjEE8aCXVzLXdlc3QtMiJHMEUCIHM+wQa9lGvIJkJhHifj+gDt8yzgp7wivgXV6Ta13gjWAiEAibxXdNA9odsYQNxx8X2jX+f3mTqPezidqxlqLQCKpuwqqQIIeBAAGgw3MjY4OTg4MDYzMzkiDBreo4KN1OBcKRLQ2iqGAhS8hif9gh+x+F6rEQA1/Sm17kmWAj7wpUGfo53Ju6q2Ailt4BECmuBQP1NNh4SJHNCh6N+tH4Dw/FQbcBbf6mBPzzD1tTd13XkorX8TO+WVPpqy5oIBC6qg4mqk/tAjvex/0Jk9LrQzVflcrgVXWegKFPXrPBj286pMtbLdlZN6cRhdrGrKzz3Fpsy8hu6/+N45xAGnaj7LTbD9+EWAOh6ZAIrprSZ98DnRkcBn9bTXFWU4NnXM1mPWNIEynxo0DzUEzj3DmdnGOIbOhs9/A6kdg73JiAvuSDj0EnI8BUUx7IcvgsKELcj9wtD8He4ZNncuFeyo4kTFOS9IxEUHX7iOopWDa7QwnOC1sAY6nQF/9/QNtxrhlioeuX+EDqHELYduuipyf7EsmAAlx2w0vYNq7O0vw7rVKM+XlUlfR651S4Syg2lWGzc6ZvCALuam+4ePet89xhlkHj2YxjXJ9KZc5rxQxugckFsGXU/dHuMMp+xWj6jpIrwVBseIzfjNwLIaVlVFgDYRV0RyDTtHqrLNpLQqwhpUioPKOxSWzAoOsjpzlNtpf41ImQ8x',
                          region_name='us-east-1')
    endpoint_name = 'sklearn-local-ep2024-04-03-17-28-19'
    content_type = 'application/json'  # The MIME type of the input data in the request body.
    accept = 'application/json'  # The desired MIME type of the inference in the response.
    response = client.invoke_endpoint(
        EndpointName=endpoint_name,
        ContentType=content_type,
        Accept=accept,
        Body=payload_data
    )

    response_body = response['Body'].read().decode('utf-8')
    response_data = json.loads(response_body)

    return response_data

# Set the page configuration for better resolution
st.set_page_config(page_title="Classification Results", layout="wide")

# Create Streamlit app
st.title('Sagemaker Endpoint Classifier')

# File uploader for uploading files
uploaded_files = st.file_uploader("Upload Exe Files" , accept_multiple_files=True)

number = st.number_input("Enter a number of samples to be checked (For quick demo choose small value)", min_value=1, max_value=200, step=1)

if uploaded_files:
    # Display uploaded file name
    st.text("Uploaded files:")
    file_names = [file.name for file in uploaded_files]
    st.text(', '.join(file_names))

    # Button to classify
    if st.button("Classify"):
        y_pred = []

        importsCorpus_test = []
        numSections_test = []
        sectionNames_test = []
        NgramFeaturesList_test = []

        imports_featurizer = joblib.load('imports_featurizer')
        section_names_featurizer = joblib.load('section_names_featurizer')

        
        importsCorpus_test = []
        numSections_test = []
        sectionNames_test = []
        NgramFeaturesList_test = []
        y_test = []
        for i in range(len(file_names)):
            file = file_names[i]
            print(file)
            try:
                NGramFeatures = getNGramFeaturesFromSample(file, K1_most_common_Ngrams_list)
                pe = pefile.PE(file)
                imports = getImports(pe)
                nSections = len(pe.sections)
                secNames = getSectionNames(pe)
                importsCorpus_test.append(imports)
                numSections_test.append(nSections)
                sectionNames_test.append(secNames)
                NgramFeaturesList_test.append(NGramFeatures)
                print(importsCorpus_test)
                print(numSections_test)
                print(sectionNames_test)
                print(NgramFeaturesList_test)
            except Exception as e:
                print(file+":")
                print(e)

        importsCorpus_test_transformed = imports_featurizer.transform(importsCorpus_test)
        sectionNames_test_transformed = section_names_featurizer.transform(sectionNames_test)
        X_test = hstack([NgramFeaturesList_test, importsCorpus_test_transformed,sectionNames_test_transformed, csr_matrix(numSections_test).transpose()])

        if number > len(file_names):
            number = len(file_names)

        for i in range(number):
            payload = {"Input": [X_test.toarray()[i].tolist()]}

            # Perform classification
            result = classify(json.dumps(payload)) 
            res = ""
            if result['Output'] == 1:
                res = "It's a Malware"
            else:
                res ="It's a Benign Software"

            y_pred.append(res)

        # Display classification results
        st.title("Classification Results")
        df = pd.DataFrame(zip(file_names, y_pred), columns=["True Label", "Predicted Label"])
        st.dataframe(df.style.set_properties(**{'text-align': 'left'}), height=500)

